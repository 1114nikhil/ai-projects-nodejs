## Build AI project with Node.js,OpenAI, LAngChain,Vector DB, HuggingFace, TypeScript 

## 1. OpenAI fundamentals

### 1.1. Open AI setup
-OpenAI key setup
    - https://platform.openai.com/
-Create a node Npm project
    -`npm init -y`
    -`npm install openai`
-run a prompt

### 1.2. TypeScript setup
- `npm i -D typescript ts-node @types/node`
-`npx tsc --init`

## Just put `start` startts or startjs will not work
    - "startjs": "node --env-file=.env src/index.js",
    - "start": "node -r ts-node/register --env-file=.env src/index.ts",


## to check no. of tokens in nodejs project
    -`npm i tiktoken`

## Roles
    -system
    -user
    -asistance

## OpenA api and playground parameters
    - Temperature
        higher Temperature will results in more random predections, lower Temperature will results in more Predictable predictions
        recomended range : 0.1 to 1.0
    - Top P
        higher top_p will results in more diverse pridiction,while a lower top_p will results in more focused predictions
        recomended range: 0.1 to 0.9
    - Maximum Tokens
        contorls the max no. of output tokens that model will generate in a single response
        recomended range : 16 to 2048
        max_output_tokens': integer below minimum value. Expected a value >= 16, but got 10 instead.
    - Frequnecy panality
        parameter controls the modal tendecy to repeat  token  that have already been generated,higher 
    - n
        number of choises

## 2 Simple Chat project
    - Chat with the OpenAI
    - add Chat history (context)
    - set token limit

### 2.1 Setup new chat-app project
    - `npm init -y`
    - `npm i -D typescript ts-node @types/node`
    - `npx tsc --init`
    - `npm i openai tiktoken` 

### Build chat with openAI
    - we will use console as the chat interface 
        - use `process.stdin.addListener`

### 2.3 add chat history (context)
    - you can build this by simple pushing the user and asistance in the array

### 2.4 Set the token limit
    - why keepp context small?
        - to control the cost 
        - to contorl the length of the response
        - to avoid error  due to large context of gpt-4.1-nano-2025-04-14 has limit of 1,047,576 context window

